# Brain Tumor Classification Model using Transfer Learning with ResNet-152

This repository contains the code for a brain tumor classification model built using the ResNet-152 architecture in PyTorch. The model is trained to classify different types of brain tumors from MRI images.


The model is trained on a collection of datasets which were augmented into a one huge chunk  Dataset,to replicate , you can add your own dataset pertinent to your own classification problem  and place in the appropriate directory in google colab. The dataset is organized into folders, with each folder representing a different class of brain tumor.

## Model Architecture

The model uses a pre-trained ResNet-152 from `torchvision.models`. The final fully connected layer is replaced with a new sequential layer to adapt to the number of classes in the brain tumor dataset. The architecture of the new head is:

- Linear layer (input features from ResNet-152, output 512)
- ReLU activation
- Dropout (p=0.5)
- Linear layer (input 512, output num_classes)

## Training

This project uses a novel training strategy to maximize the performance of the ResNet-152 model on the brain tumor dataset. The core of this approach is the use of a custom-implemented Ranger optimizer, which synergizes multiple advanced optimization techniques.

### The Ranger Optimizer

The RangerOptimizer used in this notebook is a powerful, custom implementation that combines two modern concepts:

- **RAdam (Rectified Adam):** A variant of the Adam optimizer that introduces a term to rectify the variance of the adaptive learning rate. This helps prevent issues during the warm-up phase of training, leading to more stable and reliable convergence.

- **LookAhead:** An optimization technique that works as a wrapper around an inner optimizer (RAdam in this case). It maintains a set of "slow weights" and updates them by looking ahead at the sequence of "fast weights" generated by RAdam. This approach helps in escaping local minima and improves generalization.

### Gradient Centralization (GC)

In addition to the Ranger optimizer, this implementation incorporates Gradient Centralization. GC is a simple and effective technique that operates directly on the gradients by centralizing them to have a zero mean. This technique can:

- Smooth the optimization landscape
- Regularize the weight space, acting as a form of implicit regularization
- Improve the generalization performance of the deep neural network, especially in computer vision tasks

The custom optimizer class allows for applying GC to both convolutional and fully-connected layers, providing an extra layer of optimization for better performance.

### Fine-Tuning Strategy

The overall training process involves:

- **Loss Function:** Standard Cross-Entropy Loss, suitable for this multi-class classification problem
- **Learning Rate:** A low learning rate of 1e-5 is used for fine-tuning the pre-trained ResNet-152 weights
- **Data Augmentation:** The training data is augmented with random horizontal flips and rotations to create a more robust model that is invariant to these transformations

This combination of a powerful pre-trained architecture (ResNet-152) with the advanced Ranger optimizer and Gradient Centralization provides a novel and effective method for this complex medical image classification task.

## Evaluation

The model's performance is evaluated on the validation set using:

- **Accuracy:** Overall and per-class accuracy
- **Precision, Recall, and F1-Score:** Calculated for each class
- **Confusion Matrix:** A normalized confusion matrix is plotted to visualize the classification performance across all classes

## Results

The model achieves high accuracy on the validation set, demonstrating its effectiveness in classifying brain tumors. The final validation accuracy is approximately **98.29%**. Detailed per-class metrics and the confusion matrix can be found in the evaluation section of the notebook.
